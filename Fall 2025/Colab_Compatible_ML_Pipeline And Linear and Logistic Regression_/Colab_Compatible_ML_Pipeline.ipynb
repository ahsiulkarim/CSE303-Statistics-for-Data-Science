{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "End-to-end analytical ML pipeline (binary / multi-class classification).\n",
        "Code prepared by Dr Abdullahi Chowdhury\n",
        "Presented by: Dr Abdullahi Chowdhury and Dr Manzurul Islam, Next Generation Artificial Intelligence Research Centre\n",
        "\n",
        "UPDATED FOR GOOGLE COLAB\n",
        "------------------------\n",
        "This version automatically mounts Google Drive and saves inputs/outputs there.\n",
        "\n",
        "INSTRUCTIONS\n",
        "------------\n",
        "1. Run this cell.\n",
        "2. Allow Google Drive mounting when prompted.\n",
        "3. The script will create a folder '/content/drive/MyDrive/Analytical_Project_Data'.\n",
        "4. Upload your dataset (CSV/Excel) to that folder in your Drive.\n",
        "5. Update the 'dataset_filename' in the main() function at the bottom if needed.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, Any, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Colab-specific setup for Google Drive\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"[INFO] Google Colab detected. Mounting Drive...\")\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Define the base path for input/output in Drive\n",
        "    # This creates a specific folder for your project to keep things organized\n",
        "    BASE_PATH = \"/content/drive/MyDrive/Analytical_Project_Data\"\n",
        "\n",
        "    if not os.path.exists(BASE_PATH):\n",
        "        os.makedirs(BASE_PATH)\n",
        "        print(f\"[INFO] Created working directory in Drive: {BASE_PATH}\")\n",
        "        print(f\"[ACTION REQUIRED] Please upload your dataset to: {BASE_PATH}\")\n",
        "    else:\n",
        "        print(f\"[INFO] Using existing working directory in Drive: {BASE_PATH}\")\n",
        "\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"[INFO] Not running in Colab. Using current directory.\")\n",
        "    BASE_PATH = \".\"\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import (\n",
        "    StandardScaler,\n",
        "    MinMaxScaler,\n",
        "    OneHotEncoder,\n",
        "    PolynomialFeatures\n",
        ")\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    classification_report\n",
        ")\n",
        "from sklearn.pipeline import Pipeline as SkPipeline\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
        "\n",
        "# Baseline models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "# Optional: XGBoost (will be conditionally used)\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    HAS_XGBOOST = True\n",
        "except ImportError:  # pragma: no cover - optional dependency\n",
        "    HAS_XGBOOST = False\n",
        "    XGBClassifier = None\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # We set defaults here, but they will be overridden in main() with full paths\n",
        "    file_path: str = \"heart.csv\"\n",
        "    sheet_name: Optional[str] = None      # If Excel, e.g. \"Sheet1\"\n",
        "    target_column: str = \"target\"\n",
        "    test_size: float = 0.2\n",
        "    random_state: int = 42\n",
        "    scaling_method: str = \"standard\"      # \"standard\" or \"minmax\"\n",
        "    cv_folds: int = 5\n",
        "    results_csv: str = \"experiment_results.csv\"\n",
        "    top_n_to_print: int = 10\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# DATA LOADING AND BASIC INSPECTION\n",
        "# ============================================================================\n",
        "\n",
        "def load_dataset(cfg: Config) -> pd.DataFrame:\n",
        "    \"\"\"Load dataset from CSV or Excel depending on file extension.\"\"\"\n",
        "    if not os.path.exists(cfg.file_path):\n",
        "        raise FileNotFoundError(\n",
        "            f\"Dataset not found at: {cfg.file_path}\\n\"\n",
        "            f\"If using Colab, ensure you uploaded the file to: {os.path.dirname(cfg.file_path)}\"\n",
        "        )\n",
        "\n",
        "    ext = os.path.splitext(cfg.file_path)[1].lower()\n",
        "    if ext in [\".csv\"]:\n",
        "        df = pd.read_csv(cfg.file_path)\n",
        "    elif ext in [\".xlsx\", \".xls\"]:\n",
        "        df = pd.read_excel(cfg.file_path, sheet_name=cfg.sheet_name)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file extension: {ext}\")\n",
        "\n",
        "    print(\"\\n=== Data Head ===\")\n",
        "    print(df.head())\n",
        "    return df\n",
        "\n",
        "\n",
        "def split_features_target(df: pd.DataFrame, target_column: str) -> Tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"Separate features X and target y based on target column.\"\"\"\n",
        "    if target_column not in df.columns:\n",
        "        raise KeyError(f\"Target column '{target_column}' not found in dataset. Available columns: {list(df.columns)}\")\n",
        "    X = df.drop(columns=[target_column])\n",
        "    y = df[target_column]\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def summarise_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Summarise missing values per feature (count and percentage).\"\"\"\n",
        "    missing_count = df.isna().sum()\n",
        "    missing_pct = (missing_count / len(df)) * 100\n",
        "    summary = pd.DataFrame(\n",
        "        {\n",
        "            \"column\": df.columns,\n",
        "            \"missing_count\": missing_count.values,\n",
        "            \"missing_pct\": missing_pct.values,\n",
        "        }\n",
        "    )\n",
        "    print(\"\\n=== Missing Values Summary ===\")\n",
        "    print(summary.sort_values(\"missing_pct\", ascending=False))\n",
        "    return summary\n",
        "\n",
        "\n",
        "def detect_preprocessing_needs(df: pd.DataFrame, target_column: str) -> None:\n",
        "    \"\"\"Identify basic preprocessing needs and print a short summary.\"\"\"\n",
        "    print(\"\\n=== Preprocessing Needs Summary ===\")\n",
        "    dtypes = df.dtypes\n",
        "    numeric_like_str_cols = []\n",
        "    scaling_candidates = []\n",
        "    for col, dtype in dtypes.items():\n",
        "        if col == target_column:\n",
        "            continue\n",
        "        if dtype == \"object\":\n",
        "            # crude heuristic: if most values look numeric, flag it\n",
        "            sample = df[col].dropna().astype(str).head(50)\n",
        "            if len(sample) > 0:\n",
        "                numeric_like = sample.str.match(r\"^-?\\d+(\\.\\d+)?$\").mean()\n",
        "                if numeric_like > 0.7:\n",
        "                    numeric_like_str_cols.append(col)\n",
        "        elif np.issubdtype(dtype, np.number):\n",
        "            scaling_candidates.append(col)\n",
        "\n",
        "    print(f\"Object columns that appear numeric and may need casting: {numeric_like_str_cols}\")\n",
        "    print(f\"Numeric columns that may benefit from scaling: {scaling_candidates}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PREPROCESSING AND FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "\n",
        "def get_feature_types(X: pd.DataFrame) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"Return lists of numeric and categorical feature names.\"\"\"\n",
        "    numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    categorical_features = [c for c in X.columns if c not in numeric_features]\n",
        "    return numeric_features, categorical_features\n",
        "\n",
        "\n",
        "def build_preprocessor(\n",
        "    X: pd.DataFrame,\n",
        "    cfg: Config,\n",
        "    feature_engineering: str = \"none\"\n",
        ") -> ColumnTransformer:\n",
        "    \"\"\"\n",
        "    Build a ColumnTransformer for preprocessing and simple feature engineering.\n",
        "\n",
        "    feature_engineering options:\n",
        "    - \"none\": standard numeric + categorical preprocessing\n",
        "    - \"poly\": polynomial features on numeric data\n",
        "    - \"interactions\": interaction-only polynomial features on numeric data\n",
        "    - \"pca\": PCA on numeric data after scaling (dimensionality reduction)\n",
        "    \"\"\"\n",
        "    numeric_features, categorical_features = get_feature_types(X)\n",
        "\n",
        "    if cfg.scaling_method == \"standard\":\n",
        "        scaler = StandardScaler()\n",
        "    else:\n",
        "        scaler = MinMaxScaler()\n",
        "\n",
        "    numeric_steps = [(\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
        "    cat_steps = [\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
        "    ]\n",
        "\n",
        "    if feature_engineering == \"poly\":\n",
        "        numeric_steps.append(\n",
        "            (\"poly\", PolynomialFeatures(degree=2, include_bias=False))\n",
        "        )\n",
        "        numeric_steps.append((\"scaler\", scaler))\n",
        "    elif feature_engineering == \"interactions\":\n",
        "        numeric_steps.append(\n",
        "            (\"poly\", PolynomialFeatures(degree=2, include_bias=False, interaction_only=True))\n",
        "        )\n",
        "        numeric_steps.append((\"scaler\", scaler))\n",
        "    elif feature_engineering == \"pca\":\n",
        "        # Impute -> scale -> PCA\n",
        "        numeric_steps.append((\"scaler\", scaler))\n",
        "        numeric_steps.append((\"pca\", PCA(n_components=0.95, random_state=cfg.random_state)))\n",
        "    else:  # \"none\"\n",
        "        numeric_steps.append((\"scaler\", scaler))\n",
        "\n",
        "    numeric_transformer = SkPipeline(steps=numeric_steps)\n",
        "    categorical_transformer = SkPipeline(steps=cat_steps)\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", numeric_transformer, numeric_features),\n",
        "            (\"cat\", categorical_transformer, categorical_features),\n",
        "        ]\n",
        "    )\n",
        "    return preprocessor\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CLASS IMBALANCE AND OVERSAMPLING\n",
        "# ============================================================================\n",
        "\n",
        "def compute_class_balance(y: pd.Series) -> pd.Series:\n",
        "    \"\"\"Print and return class distribution.\"\"\"\n",
        "    print(\"\\n=== Target Class Distribution ===\")\n",
        "    counts = y.value_counts()\n",
        "    print(counts)\n",
        "    return counts\n",
        "\n",
        "\n",
        "def is_imbalanced(y: pd.Series, threshold: float = 0.3) -> bool:\n",
        "    \"\"\"\n",
        "    Determine whether the dataset is imbalanced.\n",
        "    Rule: if any class proportion is < threshold, treat as imbalanced.\n",
        "    \"\"\"\n",
        "    value_counts = y.value_counts(normalize=True)\n",
        "    minority_ratio = value_counts.min()\n",
        "    return minority_ratio < threshold\n",
        "\n",
        "\n",
        "def get_oversamplers(cfg: Config) -> Dict[str, Any]:\n",
        "    \"\"\"Return oversampler objects keyed by name.\"\"\"\n",
        "    oversamplers = {\n",
        "        \"none\": None,\n",
        "        \"SMOTE\": SMOTE(random_state=cfg.random_state),\n",
        "        \"ADASYN\": ADASYN(random_state=cfg.random_state),\n",
        "        \"BorderlineSMOTE\": BorderlineSMOTE(random_state=cfg.random_state, kind=\"borderline-1\")\n",
        "    }\n",
        "    return oversamplers\n",
        "\n",
        "\n",
        "def get_oversampler_param_grids() -> Dict[str, Dict[str, List[Any]]]:\n",
        "    \"\"\"\n",
        "    Parameter grids for oversamplers, referenced via 'sampler__' prefix in pipelines.\n",
        "    \"\"\"\n",
        "    oversampler_grids = {\n",
        "        \"none\": {},\n",
        "        \"SMOTE\": {\n",
        "            \"sampler__sampling_strategy\": [0.5, 0.75, 1.0],\n",
        "            \"sampler__k_neighbors\": [3, 5, 7],\n",
        "        },\n",
        "        \"ADASYN\": {\n",
        "            \"sampler__sampling_strategy\": [0.5, 0.75, 1.0],\n",
        "            \"sampler__n_neighbors\": [3, 5, 7],\n",
        "        },\n",
        "        \"BorderlineSMOTE\": {\n",
        "            \"sampler__sampling_strategy\": [0.5, 0.75, 1.0],\n",
        "            \"sampler__k_neighbors\": [3, 5, 7],\n",
        "            \"sampler__kind\": [\"borderline-1\", \"borderline-2\"],\n",
        "        },\n",
        "    }\n",
        "    return oversampler_grids\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MODELS AND HYPERPARAMETER GRIDS\n",
        "# ============================================================================\n",
        "\n",
        "def get_baseline_models(cfg: Config) -> Dict[str, Tuple[Any, Dict[str, List[Any]]]]:\n",
        "    \"\"\"Return baseline classifiers and their parameter grids.\"\"\"\n",
        "    models = {}\n",
        "\n",
        "    # Random Forest\n",
        "    rf = RandomForestClassifier(random_state=cfg.random_state)\n",
        "    rf_grid = {\n",
        "        \"clf__n_estimators\": [100, 200],\n",
        "        \"clf__max_depth\": [None, 5, 10],\n",
        "        \"clf__min_samples_split\": [2, 5, 10],\n",
        "        \"clf__min_samples_leaf\": [1, 2, 4],\n",
        "        \"clf__max_features\": [\"sqrt\", \"log2\"],\n",
        "    }\n",
        "    models[\"RandomForest\"] = (rf, rf_grid)\n",
        "\n",
        "    # Logistic Regression\n",
        "    lr = LogisticRegression(\n",
        "        max_iter=1000,\n",
        "        solver=\"liblinear\"  # good for small-to-medium binary problems\n",
        "    )\n",
        "    lr_grid = {\n",
        "        \"clf__C\": [0.01, 0.1, 1.0, 10.0],\n",
        "        \"clf__penalty\": [\"l1\", \"l2\"],\n",
        "        \"clf__class_weight\": [None, \"balanced\"],\n",
        "        \"clf__fit_intercept\": [True, False],\n",
        "    }\n",
        "    models[\"LogisticRegression\"] = (lr, lr_grid)\n",
        "\n",
        "    # Decision Tree\n",
        "    dt = DecisionTreeClassifier(random_state=cfg.random_state)\n",
        "    dt_grid = {\n",
        "        \"clf__max_depth\": [None, 5, 10, 20],\n",
        "        \"clf__min_samples_split\": [2, 5, 10],\n",
        "        \"clf__min_samples_leaf\": [1, 2, 4],\n",
        "        \"clf__criterion\": [\"gini\", \"entropy\"],\n",
        "        \"clf__max_features\": [None, \"sqrt\", \"log2\"],\n",
        "    }\n",
        "    models[\"DecisionTree\"] = (dt, dt_grid)\n",
        "\n",
        "    # Support Vector Machine (SVC with probability for ROC AUC)\n",
        "    svm = SVC(probability=True, random_state=cfg.random_state)\n",
        "    svm_grid = {\n",
        "        \"clf__C\": [0.1, 1, 10],\n",
        "        \"clf__kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
        "        \"clf__gamma\": [\"scale\", \"auto\"],\n",
        "        \"clf__degree\": [2, 3],\n",
        "        \"clf__class_weight\": [None, \"balanced\"],\n",
        "    }\n",
        "    models[\"SVM\"] = (svm, svm_grid)\n",
        "\n",
        "    return models\n",
        "\n",
        "\n",
        "def get_ensemble_models(cfg: Config) -> Dict[str, Tuple[Any, Dict[str, List[Any]]]]:\n",
        "    \"\"\"Return ensemble models and their parameter grids.\"\"\"\n",
        "    models = {}\n",
        "\n",
        "    # Random Forest again, but treated explicitly as an ensemble\n",
        "    rf = RandomForestClassifier(random_state=cfg.random_state)\n",
        "    rf_grid = {\n",
        "        \"clf__n_estimators\": [100, 200],\n",
        "        \"clf__max_depth\": [None, 5, 10],\n",
        "        \"clf__min_samples_split\": [2, 5, 10],\n",
        "        \"clf__min_samples_leaf\": [1, 2, 4],\n",
        "        \"clf__max_features\": [\"sqrt\", \"log2\"],\n",
        "    }\n",
        "    models[\"Ensemble_RandomForest\"] = (rf, rf_grid)\n",
        "\n",
        "    # Gradient Boosting\n",
        "    gb = GradientBoostingClassifier(random_state=cfg.random_state)\n",
        "    gb_grid = {\n",
        "        \"clf__n_estimators\": [100, 200],\n",
        "        \"clf__learning_rate\": [0.01, 0.1],\n",
        "        \"clf__max_depth\": [3, 5],\n",
        "        \"clf__min_samples_split\": [2, 5],\n",
        "        \"clf__min_samples_leaf\": [1, 2],\n",
        "    }\n",
        "    models[\"GradientBoosting\"] = (gb, gb_grid)\n",
        "\n",
        "    # XGBoost if available\n",
        "    if HAS_XGBOOST:\n",
        "        xgb = XGBClassifier(\n",
        "            random_state=cfg.random_state,\n",
        "            use_label_encoder=False,\n",
        "            eval_metric=\"logloss\"\n",
        "        )\n",
        "        xgb_grid = {\n",
        "            \"clf__n_estimators\": [100, 200],\n",
        "            \"clf__max_depth\": [3, 5, 7],\n",
        "            \"clf__learning_rate\": [0.01, 0.1],\n",
        "            \"clf__subsample\": [0.8, 1.0],\n",
        "            \"clf__colsample_bytree\": [0.8, 1.0],\n",
        "        }\n",
        "        models[\"XGBoost\"] = (xgb, xgb_grid)\n",
        "    else:\n",
        "        print(\"\\n[INFO] xgboost is not installed. XGBoost will be skipped.\")\n",
        "\n",
        "    return models\n",
        "\n",
        "\n",
        "def get_stacking_model(cfg: Config) -> Tuple[StackingClassifier, Dict[str, List[Any]]]:\n",
        "    \"\"\"Build a stacking classifier and parameter grid.\"\"\"\n",
        "    base_estimators = [\n",
        "        (\"rf\", RandomForestClassifier(random_state=cfg.random_state)),\n",
        "        (\"svm\", SVC(probability=True, random_state=cfg.random_state)),\n",
        "        (\"lr\", LogisticRegression(max_iter=1000, solver=\"liblinear\")),\n",
        "        (\"gb\", GradientBoostingClassifier(random_state=cfg.random_state)),\n",
        "    ]\n",
        "    final_estimator = LogisticRegression(max_iter=1000, solver=\"liblinear\")\n",
        "\n",
        "    stack_clf = StackingClassifier(\n",
        "        estimators=base_estimators,\n",
        "        final_estimator=final_estimator,\n",
        "        passthrough=True\n",
        "    )\n",
        "\n",
        "    param_grid = {\n",
        "        \"clf__final_estimator__C\": [0.1, 1.0, 10.0],\n",
        "        \"clf__final_estimator__penalty\": [\"l1\", \"l2\"],\n",
        "        \"clf__stack_method\": [\"auto\", \"predict_proba\"],\n",
        "        \"clf__cv\": [3, 5],\n",
        "        \"clf__n_jobs\": [None],\n",
        "    }\n",
        "\n",
        "    return stack_clf, param_grid\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# METRICS\n",
        "# ============================================================================\n",
        "\n",
        "def infer_positive_label(y: pd.Series) -> Any:\n",
        "    \"\"\"\n",
        "    Choose a positive class label.\n",
        "    For binary classification, use the minority class.\n",
        "    For multi-class, use the least frequent class (still defined for TPR/FNR\n",
        "    on that class only).\n",
        "    \"\"\"\n",
        "    counts = y.value_counts()\n",
        "    positive_label = counts.idxmin()\n",
        "    return positive_label\n",
        "\n",
        "\n",
        "def compute_metrics(\n",
        "    y_true: np.ndarray,\n",
        "    y_pred: np.ndarray,\n",
        "    y_proba: Optional[np.ndarray],\n",
        "    positive_label: Any\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Compute accuracy, confusion matrix, TPR, FNR, TP, FP, ROC AUC.\"\"\"\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    labels = np.unique(y_true)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "    # Attempt to map TP/FP/FN/TN when binary or using positive_label\n",
        "    if len(labels) == 2:\n",
        "        # Ensure we know which index is positive\n",
        "        pos_index = list(labels).index(positive_label)\n",
        "        # For binary confusion matrix:\n",
        "        # rows = true, cols = pred\n",
        "        # If labels=[neg,pos], cm:\n",
        "        # [[TN, FP],\n",
        "        #  [FN, TP]]\n",
        "        if pos_index == 1:\n",
        "            tn, fp, fn, tp = cm.ravel()\n",
        "        else:\n",
        "            # labels order is [pos, neg], re-interpret accordingly\n",
        "            tp = cm[0, 0]\n",
        "            fn = cm[0, 1]\n",
        "            fp = cm[1, 0]\n",
        "            tn = cm[1, 1]\n",
        "    else:\n",
        "        # For multi-class, TP/FP/FN/TN not straightforward as scalars;\n",
        "        # set to NaN and document in results.\n",
        "        tp = fp = fn = tn = np.nan\n",
        "\n",
        "    # TPR and FNR\n",
        "    if len(labels) == 2:\n",
        "        # recall for positive class\n",
        "        if (tp + fn) > 0:\n",
        "            tpr = tp / (tp + fn)\n",
        "            fnr = fn / (tp + fn)\n",
        "        else:\n",
        "            tpr = np.nan\n",
        "            fnr = np.nan\n",
        "    else:\n",
        "        tpr = np.nan\n",
        "        fnr = np.nan\n",
        "\n",
        "    # ROC AUC\n",
        "    roc = np.nan\n",
        "    if y_proba is not None:\n",
        "        try:\n",
        "            if len(labels) == 2:\n",
        "                # y_proba is probability of positive class\n",
        "                roc = roc_auc_score(y_true, y_proba)\n",
        "            else:\n",
        "                # multi-class (one-vs-rest)\n",
        "                roc = roc_auc_score(y_true, y_proba, multi_class=\"ovr\")\n",
        "        except Exception:\n",
        "            roc = np.nan\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": acc,\n",
        "        \"tpr\": tpr,\n",
        "        \"fnr\": fnr,\n",
        "        \"tp\": tp,\n",
        "        \"fp\": fp,\n",
        "        \"tn\": tn,\n",
        "        \"fn\": fn,\n",
        "        \"roc_auc\": roc,\n",
        "        \"confusion_matrix\": cm.tolist(),  # store as list for CSV/json\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING AND EVALUATION ROUTINES\n",
        "# ============================================================================\n",
        "\n",
        "def run_grid_search(\n",
        "    pipeline,\n",
        "    param_grid: Dict[str, List[Any]],\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    cfg: Config\n",
        ") -> GridSearchCV:\n",
        "    \"\"\"Run GridSearchCV for a given pipeline and hyperparameter grid.\"\"\"\n",
        "    cv = StratifiedKFold(n_splits=cfg.cv_folds, shuffle=True, random_state=cfg.random_state)\n",
        "    gs = GridSearchCV(\n",
        "        estimator=pipeline,\n",
        "        param_grid=param_grid,\n",
        "        cv=cv,\n",
        "        scoring=\"roc_auc\",\n",
        "        n_jobs=-1,\n",
        "        verbose=0\n",
        "    )\n",
        "    gs.fit(X_train, y_train)\n",
        "    return gs\n",
        "\n",
        "\n",
        "def evaluate_and_record(\n",
        "    results: List[Dict[str, Any]],\n",
        "    model_name: str,\n",
        "    model_category: str,\n",
        "    feature_engineering: str,\n",
        "    oversampler_name: str,\n",
        "    grid_search: GridSearchCV,\n",
        "    X_test: pd.DataFrame,\n",
        "    y_test: pd.Series,\n",
        "    positive_label: Any,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Evaluate best estimator on test set, compute metrics and record results\n",
        "    in the provided list of dictionaries.\n",
        "    \"\"\"\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Predictions and probabilities\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    try:\n",
        "        proba = best_model.predict_proba(X_test)\n",
        "        # assume binary: need prob of positive class\n",
        "        if proba.shape[1] == 2:\n",
        "            y_proba_pos = proba[:, list(best_model.classes_).index(positive_label)]\n",
        "        else:\n",
        "            y_proba_pos = proba\n",
        "    except Exception:\n",
        "        y_proba_pos = None\n",
        "\n",
        "    metrics = compute_metrics(y_test, y_pred, y_proba_pos, positive_label)\n",
        "    result_entry = {\n",
        "        \"model_name\": model_name,\n",
        "        \"model_category\": model_category,\n",
        "        \"feature_engineering\": feature_engineering,\n",
        "        \"oversampler\": oversampler_name,\n",
        "        \"best_params\": best_params,\n",
        "    }\n",
        "    result_entry.update(metrics)\n",
        "    results.append(result_entry)\n",
        "\n",
        "\n",
        "def run_model_suite(\n",
        "    cfg: Config,\n",
        "    X_train: pd.DataFrame,\n",
        "    X_test: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    y_test: pd.Series,\n",
        "    feature_engineering_methods: List[str],\n",
        "    baseline_models: Dict[str, Tuple[Any, Dict[str, List[Any]]]],\n",
        "    ensemble_models: Dict[str, Tuple[Any, Dict[str, List[Any]]]],\n",
        "    stacking_model: Tuple[Any, Dict[str, List[Any]]],\n",
        "    y_train_full: pd.Series,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Run baseline models, feature-engineered models, oversampling, ensembles,\n",
        "    and stacking. Return a DataFrame with all results.\n",
        "    \"\"\"\n",
        "    results: List[Dict[str, Any]] = []\n",
        "    positive_label = infer_positive_label(y_train_full)\n",
        "    print(f\"\\n[INFO] Positive label used for TPR/FNR: {positive_label}\")\n",
        "\n",
        "    oversamplers = get_oversamplers(cfg)\n",
        "    oversampler_grids = get_oversampler_param_grids()\n",
        "\n",
        "    # Check if dataset is imbalanced\n",
        "    imbalance_flag = is_imbalanced(y_train_full)\n",
        "    print(f\"\\n[INFO] Dataset imbalanced? {imbalance_flag}\")\n",
        "\n",
        "    # Always run the \"none\" oversampling setting (original data)\n",
        "    oversampling_to_run = [\"none\"]\n",
        "    if imbalance_flag:\n",
        "        oversampling_to_run.extend([\"SMOTE\", \"ADASYN\", \"BorderlineSMOTE\"])\n",
        "\n",
        "    # ============================================================== #\n",
        "    #  BASELINE + FEATURE ENGINEERING + OVERSAMPLING                 #\n",
        "    # ============================================================== #\n",
        "\n",
        "    for fe_method in feature_engineering_methods:\n",
        "        print(f\"\\n=== Feature Engineering: {fe_method} ===\")\n",
        "        preprocessor = build_preprocessor(X_train, cfg, feature_engineering=fe_method)\n",
        "\n",
        "        for oversampler_name in oversampling_to_run:\n",
        "            print(f\"\\n--- Oversampler: {oversampler_name} ---\")\n",
        "            sampler = oversamplers[oversampler_name]\n",
        "            sampler_grid = oversampler_grids[oversampler_name]\n",
        "\n",
        "            for model_name, (clf, param_grid) in baseline_models.items():\n",
        "                print(f\"\\n[Baseline] Training model: {model_name}\")\n",
        "                # Build pipeline\n",
        "                if sampler is None:\n",
        "                    pipeline = SkPipeline(\n",
        "                        steps=[(\"preprocess\", preprocessor), (\"clf\", clf)]\n",
        "                    )\n",
        "                else:\n",
        "                    pipeline = ImbPipeline(\n",
        "                        steps=[(\"preprocess\", preprocessor), (\"sampler\", sampler), (\"clf\", clf)]\n",
        "                    )\n",
        "\n",
        "                # Merge model grid with sampler grid\n",
        "                full_grid = dict(param_grid)\n",
        "                full_grid.update(sampler_grid)\n",
        "\n",
        "                gs = run_grid_search(pipeline, full_grid, X_train, y_train, cfg)\n",
        "                evaluate_and_record(\n",
        "                    results,\n",
        "                    model_name=model_name,\n",
        "                    model_category=\"baseline\",\n",
        "                    feature_engineering=fe_method,\n",
        "                    oversampler_name=oversampler_name,\n",
        "                    grid_search=gs,\n",
        "                    X_test=X_test,\n",
        "                    y_test=y_test,\n",
        "                    positive_label=positive_label,\n",
        "                )\n",
        "\n",
        "    # ============================================================== #\n",
        "    #  ENSEMBLE METHODS                                              #\n",
        "    # ============================================================== #\n",
        "    for fe_method in feature_engineering_methods:\n",
        "        print(f\"\\n=== [Ensemble] Feature Engineering: {fe_method} ===\")\n",
        "        preprocessor = build_preprocessor(X_train, cfg, feature_engineering=fe_method)\n",
        "\n",
        "        for oversampler_name in oversampling_to_run:\n",
        "            print(f\"\\n--- [Ensemble] Oversampler: {oversampler_name} ---\")\n",
        "            sampler = oversamplers[oversampler_name]\n",
        "            sampler_grid = oversampler_grids[oversampler_name]\n",
        "\n",
        "            for model_name, (clf, param_grid) in ensemble_models.items():\n",
        "                print(f\"\\n[Ensemble] Training model: {model_name}\")\n",
        "                if sampler is None:\n",
        "                    pipeline = SkPipeline(\n",
        "                        steps=[(\"preprocess\", preprocessor), (\"clf\", clf)]\n",
        "                    )\n",
        "                else:\n",
        "                    pipeline = ImbPipeline(\n",
        "                        steps=[(\"preprocess\", preprocessor), (\"sampler\", sampler), (\"clf\", clf)]\n",
        "                    )\n",
        "\n",
        "                full_grid = dict(param_grid)\n",
        "                full_grid.update(sampler_grid)\n",
        "\n",
        "                gs = run_grid_search(pipeline, full_grid, X_train, y_train, cfg)\n",
        "                evaluate_and_record(\n",
        "                    results,\n",
        "                    model_name=model_name,\n",
        "                    model_category=\"ensemble\",\n",
        "                    feature_engineering=fe_method,\n",
        "                    oversampler_name=oversampler_name,\n",
        "                    grid_search=gs,\n",
        "                    X_test=X_test,\n",
        "                    y_test=y_test,\n",
        "                    positive_label=positive_label,\n",
        "                )\n",
        "\n",
        "    # ============================================================== #\n",
        "    #  STACKING MODEL                                                #\n",
        "    # ============================================================== #\n",
        "    stack_clf, stack_grid = stacking_model\n",
        "    for fe_method in feature_engineering_methods:\n",
        "        print(f\"\\n=== [Stacking] Feature Engineering: {fe_method} ===\")\n",
        "        preprocessor = build_preprocessor(X_train, cfg, feature_engineering=fe_method)\n",
        "\n",
        "        for oversampler_name in oversampling_to_run:\n",
        "            print(f\"\\n--- [Stacking] Oversampler: {oversampler_name} ---\")\n",
        "            sampler = oversamplers[oversampler_name]\n",
        "            sampler_grid = oversampler_grids[oversampler_name]\n",
        "\n",
        "            model_name = \"StackingClassifier\"\n",
        "            if sampler is None:\n",
        "                pipeline = SkPipeline(\n",
        "                    steps=[(\"preprocess\", preprocessor), (\"clf\", stack_clf)]\n",
        "                )\n",
        "            else:\n",
        "                pipeline = ImbPipeline(\n",
        "                    steps=[(\"preprocess\", preprocessor), (\"sampler\", sampler), (\"clf\", stack_clf)]\n",
        "                )\n",
        "\n",
        "            full_grid = dict(stack_grid)\n",
        "            full_grid.update(sampler_grid)\n",
        "\n",
        "            gs = run_grid_search(pipeline, full_grid, X_train, y_train, cfg)\n",
        "            evaluate_and_record(\n",
        "                results,\n",
        "                model_name=model_name,\n",
        "                model_category=\"stacking\",\n",
        "                feature_engineering=fe_method,\n",
        "                oversampler_name=oversampler_name,\n",
        "                grid_search=gs,\n",
        "                X_test=X_test,\n",
        "                y_test=y_test,\n",
        "                positive_label=positive_label,\n",
        "            )\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    return results_df\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN ENTRY POINT\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    # INPUT SETTINGS:\n",
        "    dataset_filename = \"heart.csv\"  # <-- Change this to your file name in Drive\n",
        "    target_col = \"target\"                  # <-- Change this to your target column\n",
        "\n",
        "    # Construct full paths based on the detected environment (Colab vs Local)\n",
        "    full_file_path = os.path.join(BASE_PATH, dataset_filename)\n",
        "    results_path = os.path.join(BASE_PATH, \"experiment_results.csv\")\n",
        "\n",
        "    print(f\"[INFO] Looking for dataset at: {full_file_path}\")\n",
        "    print(f\"[INFO] Results will be saved to: {results_path}\")\n",
        "\n",
        "    cfg = Config(\n",
        "        file_path=full_file_path,\n",
        "        sheet_name=None,               # e.g. \"Sheet1\" if Excel\n",
        "        target_column=target_col,\n",
        "        results_csv=results_path\n",
        "    )\n",
        "\n",
        "    # Load data\n",
        "    try:\n",
        "        df = load_dataset(cfg)\n",
        "    except FileNotFoundError as e:\n",
        "        print(\"\\n\" + \"!\"*50)\n",
        "        print(e)\n",
        "        print(\"!\"*50 + \"\\n\")\n",
        "        return\n",
        "\n",
        "    # Basic missing values analysis\n",
        "    _ = summarise_missing_values(df)\n",
        "\n",
        "    # Detect basic preprocessing needs\n",
        "    detect_preprocessing_needs(df, cfg.target_column)\n",
        "\n",
        "    # Split features and target\n",
        "    try:\n",
        "        X, y = split_features_target(df, cfg.target_column)\n",
        "    except KeyError as e:\n",
        "        print(f\"\\n[ERROR] {e}\")\n",
        "        return\n",
        "\n",
        "    # Class balance\n",
        "    compute_class_balance(y)\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X,\n",
        "        y,\n",
        "        test_size=cfg.test_size,\n",
        "        random_state=cfg.random_state,\n",
        "        stratify=y,\n",
        "    )\n",
        "\n",
        "    # Define feature-engineering methods to test\n",
        "    feature_engineering_methods = [\"none\", \"poly\", \"interactions\", \"pca\"]\n",
        "\n",
        "    # Baseline and ensemble models\n",
        "    baseline_models = get_baseline_models(cfg)\n",
        "    ensemble_models = get_ensemble_models(cfg)\n",
        "    stacking_model = get_stacking_model(cfg)\n",
        "\n",
        "    # Run the full suite of experiments\n",
        "    results_df = run_model_suite(\n",
        "        cfg=cfg,\n",
        "        X_train=X_train,\n",
        "        X_test=X_test,\n",
        "        y_train=y_train,\n",
        "        y_test=y_test,\n",
        "        feature_engineering_methods=feature_engineering_methods,\n",
        "        baseline_models=baseline_models,\n",
        "        ensemble_models=ensemble_models,\n",
        "        stacking_model=stacking_model,\n",
        "        y_train_full=y_train,\n",
        "    )\n",
        "\n",
        "    # Save results\n",
        "    results_df.to_csv(cfg.results_csv, index=False)\n",
        "    print(f\"\\n[INFO] All experiment results saved to: {cfg.results_csv}\")\n",
        "\n",
        "    # Print top-N models by ROC AUC, then by accuracy as tie-breaker\n",
        "    print(f\"\\n=== Top {cfg.top_n_to_print} Models by ROC AUC ===\")\n",
        "    top_results = results_df.sort_values(\n",
        "        by=[\"roc_auc\", \"accuracy\"], ascending=False\n",
        "    ).head(cfg.top_n_to_print)\n",
        "    print(top_results[[\n",
        "        \"model_name\",\n",
        "        \"model_category\",\n",
        "        \"feature_engineering\",\n",
        "        \"oversampler\",\n",
        "        \"accuracy\",\n",
        "        \"roc_auc\",\n",
        "        \"tpr\",\n",
        "        \"fnr\"\n",
        "    ]])\n",
        "\n",
        "    # Final summary\n",
        "    if not top_results.empty:\n",
        "        best_row = top_results.iloc[0]\n",
        "        print(\"\\n=== Final Summary ===\")\n",
        "        print(\"Best overall configuration:\")\n",
        "        print(\n",
        "            f\"- Model: {best_row['model_name']} ({best_row['model_category']})\\n\"\n",
        "            f\"- Feature engineering: {best_row['feature_engineering']}\\n\"\n",
        "            f\"- Oversampling: {best_row['oversampler']}\\n\"\n",
        "            f\"- Accuracy: {best_row['accuracy']:.4f}\\n\"\n",
        "            f\"- ROC AUC: {best_row['roc_auc']:.4f}\\n\"\n",
        "            f\"- TPR: {best_row['tpr']:.4f} | FNR: {best_row['fnr']:.4f}\"\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Google Colab detected. Mounting Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[INFO] Using existing working directory in Drive: /content/drive/MyDrive/Analytical_Project_Data\n",
            "[INFO] Looking for dataset at: /content/drive/MyDrive/Analytical_Project_Data/heart.csv\n",
            "[INFO] Results will be saved to: /content/drive/MyDrive/Analytical_Project_Data/experiment_results.csv\n",
            "\n",
            "=== Data Head ===\n",
            "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
            "0   52    1   0       125   212    0        1      168      0      1.0      2   \n",
            "1   53    1   0       140   203    1        0      155      1      3.1      0   \n",
            "2   70    1   0       145   174    0        1      125      1      2.6      0   \n",
            "3   61    1   0       148   203    0        1      161      0      0.0      2   \n",
            "4   62    0   0       138   294    1        1      106      0      1.9      1   \n",
            "\n",
            "   ca  thal  target  \n",
            "0   2     3       0  \n",
            "1   0     3       0  \n",
            "2   0     3       0  \n",
            "3   1     3       0  \n",
            "4   3     2       0  \n",
            "\n",
            "=== Missing Values Summary ===\n",
            "      column  missing_count  missing_pct\n",
            "0        age              0          0.0\n",
            "1        sex              0          0.0\n",
            "2         cp              0          0.0\n",
            "3   trestbps              0          0.0\n",
            "4       chol              0          0.0\n",
            "5        fbs              0          0.0\n",
            "6    restecg              0          0.0\n",
            "7    thalach              0          0.0\n",
            "8      exang              0          0.0\n",
            "9    oldpeak              0          0.0\n",
            "10     slope              0          0.0\n",
            "11        ca              0          0.0\n",
            "12      thal              0          0.0\n",
            "13    target              0          0.0\n",
            "\n",
            "=== Preprocessing Needs Summary ===\n",
            "Object columns that appear numeric and may need casting: []\n",
            "Numeric columns that may benefit from scaling: ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
            "\n",
            "=== Target Class Distribution ===\n",
            "target\n",
            "1    526\n",
            "0    499\n",
            "Name: count, dtype: int64\n",
            "\n",
            "[INFO] Positive label used for TPR/FNR: 0\n",
            "\n",
            "[INFO] Dataset imbalanced? False\n",
            "\n",
            "=== Feature Engineering: none ===\n",
            "\n",
            "--- Oversampler: none ---\n",
            "\n",
            "[Baseline] Training model: RandomForest\n",
            "\n",
            "[Baseline] Training model: LogisticRegression\n",
            "\n",
            "[Baseline] Training model: DecisionTree\n",
            "\n",
            "[Baseline] Training model: SVM\n",
            "\n",
            "=== Feature Engineering: poly ===\n",
            "\n",
            "--- Oversampler: none ---\n",
            "\n",
            "[Baseline] Training model: RandomForest\n",
            "\n",
            "[Baseline] Training model: LogisticRegression\n",
            "\n",
            "[Baseline] Training model: DecisionTree\n",
            "\n",
            "[Baseline] Training model: SVM\n",
            "\n",
            "=== Feature Engineering: interactions ===\n",
            "\n",
            "--- Oversampler: none ---\n",
            "\n",
            "[Baseline] Training model: RandomForest\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxZGM0nbxaty",
        "outputId": "42f1b783-d75a-4ec9-a399-f6175e912f01"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oWWdT_VmxmTl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}